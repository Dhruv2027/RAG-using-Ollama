# RAG-using-Ollama

This is my first project while trying to learn how Large Language Models (LLMs) can be used to perform Retrieval Augmented Generation (RAG) on a custom dataset. In this example, the popular book "Peter Pan" is used, and the user can ask the model any questions based on the content of the book. Here, the platform "Ollama" is used to run the "mistral" chatbot model and the "nomic-embed-text" model to create the vector embeddings, based on which the chatbot answers the user's questions.
